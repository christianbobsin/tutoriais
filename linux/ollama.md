
# ü¶ô Guia r√°pido do Ollama no Ubuntu

Vers√£o 1.0.0 - 05/12/2025


Pequeno tutorial para usar o **Ollama no Ubuntu** (22.04+).  
Tudo aqui √© via **terminal**.

---

## ‚úÖ Como verificar se o Ollama est√° instalado

No terminal:

```bash
ollama --version
```

* Se aparecer algo como `ollama version 0.x.x`, ele est√° instalado.
* Se aparecer `command not found`, voc√™ ainda precisa instalar.

Opcional:

```bash
which ollama      # mostra o caminho do bin√°rio, se existir
ollama list       # lista modelos locais, se o servidor estiver rodando
```

---

## üì• Como instalar o Ollama no Ubuntu

1. Atualize e garanta que o `curl` esteja instalado:

```bash
sudo apt update
sudo apt install -y curl
```

2. Execute o script oficial de instala√ß√£o:

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

3. Confirme a instala√ß√£o:

```bash
ollama --version
```

---

## ‚ñ∂Ô∏è Como ativar o Ollama

Voc√™ pode rodar o Ollama de duas formas: **direto no terminal** ou como **servi√ßo em background**.

### 3.1. Modo simples (somente enquanto o terminal estiver aberto)

```bash
ollama serve
```

* O comando ‚Äúsegura‚Äù o terminal.
* Para parar, use `Ctrl + C`.

### 3.2. Modo servi√ßo (systemd, recomendado)

Se o instalador configurou o servi√ßo `ollama`:

```bash
sudo systemctl start ollama      # inicia o servi√ßo
sudo systemctl status ollama     # verifica se est√° rodando
```

Opcional para iniciar junto com o sistema:

```bash
sudo systemctl enable ollama
```

---

## üìö Como listar modelos (locais)

Modelos **j√° baixados** (no seu PC):

```bash
ollama list
# ou
ollama ls
```

Exemplo de sa√≠da:

```text
NAME           ID        SIZE   MODIFIED
llama3.2:3b    ...       2.5 GB 2 hours ago
mistral:7b     ...       4.1 GB 1 day ago
```

---

## ‚è¨ Como instalar modelos (baixar para o PC)

Use `ollama pull`:

```bash
# Exemplo: baixar o Llama 3.2
ollama pull llama3.2
```

Ou simplesmente:

```bash
ollama run llama3.2
```

Se o modelo n√£o existir localmente, o Ollama **faz o download autom√°tico** antes de rodar.

---

## üåç Como descobrir modelos dispon√≠veis na internet

Atualmente n√£o h√° um comando tipo `ollama search` para listar todos os modelos remotos.
O caminho recomendado √© usar a **biblioteca oficial de modelos**:

üëâ **Model Library do Ollama**
[https://ollama.com/library](https://ollama.com/library)

L√° voc√™ encontra o nome exato para usar com:

```bash
ollama pull nome-do-modelo
ollama run nome-do-modelo
```

Exemplos de nomes que voc√™ pode encontrar l√°:

* `llama3.2`, `llama3.3`, `llama4`
* `mistral`, `gemma3`, `phi4`, `deepseek-r1`
* vers√µes especializadas (c√≥digo, vis√£o, etc.)

---

## üí¨ Como fazer um prompt no terminal (modo b√°sico)

### 7.1. Modo interativo

```bash
ollama run llama3.2
```

Voc√™ entra em um ‚Äúchat‚Äù no terminal:

```text
>>> Ol√°, o que √© o Ollama?
```

* Para sair: `/bye` ou `Ctrl + D`.

### 7.2. Modo ‚Äúuma pergunta e resposta‚Äù

```bash
ollama run llama3.2 "Explique o que √© um LLM em 1 frase."
```

Ou via pipe:

```bash
echo "Resuma o conceito de aprendizado de m√°quina em 2 frases." | ollama run llama3.2
```

---

## üéöÔ∏è Como fazer um prompt com par√¢metros (modo avan√ßado)

Aqui usamos flags como `--temperature`, `--top-p` e `--num-predict` para controlar o comportamento do modelo.

### 8.1. Exemplo geral com par√¢metros

```bash
ollama run llama3.2 \
  --temperature 0.4 \
  --top-p 0.9 \
  --num-predict 256 \
  "Resuma em 3 t√≥picos o que √© o Ollama."
```

A seguir, explica√ß√µes mais detalhadas de cada par√¢metro üëá

---

### 8.1.1 üî• `--temperature` (criatividade / varia√ß√£o)

Controla o quanto o modelo ‚Äúarrisca‚Äù na escolha das palavras.

* Valores **baixos (0.1‚Äì0.3)** ‚Üí respostas mais **secas, t√©cnicas e est√°veis**
* Valores **altos (0.7‚Äì1.0)** ‚Üí respostas mais **criativas e variadas**

#### Exemplo 1 ‚Äì Resumo t√©cnico

```bash
ollama run llama3.2 \
  --temperature 0.1 \
  "Explique o que √© um LLM em 2 frases, de forma t√©cnica."
```

Voc√™ deve notar:

* Respostas bem parecidas se rodar v√°rias vezes.
* Linguagem direta, pouca criatividade.

Agora, mudando s√≥ a temperatura:

```bash
ollama run llama3.2 \
  --temperature 0.9 \
  "Explique o que √© um LLM em 2 frases, de forma t√©cnica."
```

Voc√™ deve notar:

* Cada execu√ß√£o traz frases mais diferentes.
* Podem aparecer analogias e linguagem mais solta.

#### Exemplo 2 ‚Äì Texto criativo

```bash
# Baixa criatividade
ollama run llama3.2 \
  --temperature 0.2 \
  "Escreva uma frase criativa sobre caf√©."
```

Tende a vir algo simples, tipo:

> ‚ÄúCaf√© √© a bebida que nos mant√©m acordados e produtivos.‚Äù

Agora:

```bash
# Alta criatividade
ollama run llama3.2 \
  --temperature 0.9 \
  "Escreva uma frase criativa sobre caf√©."
```

Pode vir algo como:

> ‚ÄúCaf√© √© o bot√£o de ligar da alma em manh√£s que ainda est√£o em modo de espera.‚Äù

üí° Regra pr√°tica:

* **0.1‚Äì0.3** ‚Üí ideal para c√≥digo, explica√ß√µes t√©cnicas, respostas previs√≠veis.
* **0.7‚Äì1.0** ‚Üí ideal para ideias criativas, textos mais livres, brainstorming.

---

### 8.1.2 üéØ `--top-p` (controle de amostragem)

Controla **o qu√£o ‚Äúseguras‚Äù s√£o as pr√≥ximas palavras** que o modelo considera.

* `top-p` menor ‚Üí o modelo considera s√≥ as palavras mais prov√°veis (mais conservador).
* `top-p` maior ‚Üí o modelo permite alternativas menos prov√°veis (mais variedade).

#### Exemplo ‚Äì nomes para app

```bash
# top-p baixo: bem conservador
ollama run llama3.2 \
  --temperature 0.7 \
  --top-p 0.3 \
  "D√™ tr√™s sugest√µes de nomes para um app de lista de tarefas."
```

Provavelmente vir√£o nomes bem √≥bvios:

* ‚ÄúMinha Lista‚Äù
* ‚ÄúTarefas Hoje‚Äù
* ‚ÄúLista R√°pida‚Äù

Agora:

```bash
# top-p alto: mais variedade
ollama run llama3.2 \
  --temperature 0.7 \
  --top-p 0.95 \
  "D√™ tr√™s sugest√µes de nomes para um app de lista de tarefas."
```

Podem vir nomes mais variados:

* ‚ÄúCaixa de Tarefas‚Äù
* ‚ÄúChecklistar‚Äù
* ‚ÄúPronto & Feito‚Äù

üí° Regra pr√°tica:

* **0.3‚Äì0.5** ‚Üí mais seguro e previs√≠vel, bom para coisas s√©rias.
* **0.8‚Äì0.95** ‚Üí mais diversidade, bom para criatividade (especialmente com temperatura moderada/alta).

Dica:
Se a **temperatura j√° est√° baixa (0.1‚Äì0.3)**, o impacto do top-p √© menor.
Se a **temperatura √© alta (0.8‚Äì1.0)**, √© bom segurar o `top-p` em algo como **0.7‚Äì0.9** para n√£o ficar ca√≥tico.

---

### 8.1.3 ‚úÇÔ∏è `--num-predict` (tamanho m√°ximo da resposta)

Define o **m√°ximo de tokens** (peda√ßos de palavras) que o modelo pode gerar.

* Valores baixos ‚Üí respostas **curtas**.
* Valores altos ‚Üí respostas **mais longas e detalhadas**.

#### Exemplo 1 ‚Äì Resumo curto vs longo

```bash
# Resposta bem curtinha
ollama run llama3.2 \
  --num-predict 32 \
  "Explique rapidamente o que √© o Ollama."
```

Tende a dar 1‚Äì3 frases curtas.

Agora:

```bash
# Resposta mais longa
ollama run llama3.2 \
  --num-predict 256 \
  "Explique rapidamente o que √© o Ollama."
```

Aqui j√° pode virar um mini texto com v√°rios par√°grafos.

#### Exemplo 2 ‚Äì For√ßar uma sa√≠da curta (tipo t√≠tulo)

```bash
ollama run llama3.2 \
  --num-predict 16 \
  "Crie um t√≠tulo curto para um tutorial sobre Ollama no Ubuntu."
```

Isso for√ßa o modelo a n√£o escrever um par√°grafo inteiro.

üí° Regra pr√°tica:

* **16‚Äì64** ‚Üí t√≠tulos, frases curtas, comandos.
* **128‚Äì256** ‚Üí respostas padr√£o (explica√ß√µes com exemplos).
* **512+** ‚Üí textos grandes (artigos, documenta√ß√£o), mais custo de tempo e mem√≥ria.

---

### 8.2. Combos prontos (para copiar/usar)

**Uso t√©cnico / c√≥digo:**

```bash
ollama run llama3.2 \
  --temperature 0.2 \
  --top-p 0.8 \
  --num-predict 256 \
  "Explique com exemplo como usar o par√¢metro --temperature no Ollama."
```

**Brainstorm / ideias criativas:**

```bash
ollama run llama3.2 \
  --temperature 0.8 \
  --top-p 0.9 \
  --num-predict 256 \
  "Me d√™ ideias criativas de como usar o Ollama em projetos pessoais."
```

---

## üåê Como fazer prompt pela rede interna

Objetivo: usar **um PC como servidor Ollama** e acessar esse servidor de **outro PC** na mesma rede.

### 9.1. No servidor (onde o modelo est√° rodando)

Se estiver usando systemd, pare o servi√ßo para n√£o ter conflito:

```bash
sudo systemctl stop ollama  # se n√£o existir, ignore o erro
```

Inicie o servidor expondo a porta na rede:

```bash
OLLAMA_HOST=0.0.0.0:11434 ollama serve
```

Teste localmente:

```bash
curl http://localhost:11434
# deve responder algo indicando que o Ollama est√° rodando
```

### 9.2. No cliente (outro PC na mesma LAN)

Descubra o IP do servidor (ex.: `192.168.0.50`) e no cliente:

```bash
export OLLAMA_HOST=http://192.168.0.50:11434

ollama list
ollama run llama3.2 "Teste de prompt via rede interna."
```

### 9.3. Usando a API HTTP direto

```bash
curl http://192.168.0.50:11434/api/chat \
  -d '{
    "model": "llama3.2",
    "messages": [{ "role": "user", "content": "Ol√° da rede interna!" }],
    "stream": false
  }'
```

---

## üßπ Como remover modelos

Para liberar espa√ßo em disco:

```bash
# remover um modelo espec√≠fico
ollama rm llama3.2

# conferir se saiu
ollama list
```

Se precisar novamente, √© s√≥ rodar:

```bash
ollama pull llama3.2
```

---

## ‚èπÔ∏è Como desativar o Ollama

### 11.1. Se voc√™ rodou com `ollama serve` no terminal

* Use `Ctrl + C` no terminal onde est√° rodando.

### 11.2. Se estiver usando systemd

```bash
sudo systemctl stop ollama
```

Para n√£o iniciar mais com o sistema:

```bash
sudo systemctl disable ollama
```

---

## üóëÔ∏è Como remover completamente o Ollama

> ‚ö†Ô∏è Cuidado: isso remove o programa **e todos os modelos**.

1. Parar e remover o servi√ßo:

```bash
sudo systemctl stop ollama
sudo systemctl disable ollama
sudo rm /etc/systemd/system/ollama.service
sudo systemctl daemon-reload
```

2. Remover o bin√°rio:

```bash
sudo rm "$(which ollama)"
```

3. Remover libs/arquivos de instala√ß√£o (ajuste o caminho se preciso):

```bash
sudo rm -rf /usr/lib/ollama /usr/local/lib/ollama 2>/dev/null
```

4. Remover dados do usu√°rio (modelos, configs):

```bash
rm -rf ~/.ollama
```